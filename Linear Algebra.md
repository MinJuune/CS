## 1. 벡터의 내적의 정의와 이를 활용한 예시를 설명해 보세요.  

**벡터의 내적**은 두 벡터의 요소를 곱하고 합산한 값으로,  
두 벡터가 얼마나 같은 방향을 향하는지, 즉 얼마나 유사한지를 나타내는 지표입니다.  
내적은 코사인 유사도나 피어슨 상관계수 계산 등에서 유사도를 측정하는 데 활용됩니다.  

<br><br>

## 2. 선형 독립의 정의와 이를 판단하는 방법에 대해 설명해 보세요.  

**선형 독립**은 벡터 집합에서 어떤 벡터도 다른 벡터들의 선형 결합으로 표현될 수 없는 상태를 의미합니다.  
판단 방법으로는 행렬의 Rank가 벡터의 개수와 같으면 선형 독립이며, 정방 행렬인 경우에는 Determinant(행렬식)가 0이 아니면 선형 독립입니다.  

<br><br>

## 3. 기저(Basis)에 대해 설명해 보세요.  

**기저**는 어떤 공간을 생성하며(span), 동시에 서로 선형 독립인 벡터들의 집합입니다.  
즉, (1) 공간의 모든 벡터를 선형 결합으로 표현할 수 있고, (2) 어떤 벡터도 다른 벡터의 선형 결합으로 표현되지 않을 때 그 집합이 기저입니다.  

<br><br>

## 4. 계수(Rank)에 대해 설명해 보세요.  

**Rank**는 행렬의 열벡터들이 생성하는 공간의 차원, 즉 독립적인 방향의 개수를 의미합니다.  
Rank가 벡터 개수와 같으면, 모든 벡터가 선형 독립이며,  
Rank가 벡터 개수보다 작으면, 벡터들 사이에 선형 종속이 있다는 뜻입니다.  

<br><br>

## 5. 행렬식(Determinant)에 대해, 그리고 고유값과의 차이에 대해 설명해 보세요.  

**Determinant**는 선형 변환이 전체 공간의 부피를 얼마나 바꾸는지를 나타내는 값입니다.  
**고유값**은 특정 방향(고유벡터)이 변환될 때, 길이가 얼마나 스케일링 되는지를 나타내는 값입니다.  
즉, 행렬식은 전체 공간에 대한 성질이고, 고유값은 특정 방향에 대한 성질입니다.  

<br><br>

## 6. "행렬의 역행렬"의 정의와 이를 계산하는 방법을 설명해 보세요.  

**역행렬**은 행렬을 곱했을 때, 단위 행렬을 만들어주는 행렬입니다.    
역행렬은 Determinant가 0이 아닐 때만 존재하며, 이는 선형 변환이 일대일이어서 원래 벡터를 되돌릴 수 있다는 의미입니다.  
계산은 가우스 소거법이나 adjoint 공식을 이용하여 역행렬을 구할 수 있습니다.  

<br><br>

## 7. 고유값(Eigenvalue)과 고유벡터(Eigenvector)의 정의와 이들이 중요한 이유에 대해 설명해 보세요.  

**고유 벡터**는 행렬 A로 선형 변환을 해도 방향이 변하지 않는 벡터입니다.  
그리고 **고유값**은 그 고유벡터가 변환되었을 떄, 얼마나 스케일링되었는지를 나타내는 값입니다.  
(즉, A\*v = 람다\*v를 만족하는 v가 고유벡터, 람다가 고유값입니다)  
이 개념은 행렬의 성질을 이해하거나 PCA에서 데이터의 주요 방향을 찾는데 쓰입니다.  

<br><br> 

## 8. PCA에 대해 설명해 보세요.  

**PCA**는 고차원 데이터를 저차원으로 축소할 때, 정보 손실을 최소화하기 위해, 분산이 가장 큰 방향을 찾아, 그 방향으로 데이터를 투영하는 방법입니다.  
이를 위해 먼저 전체 feature 쌍의 공분산 행렬을 계산한 뒤, 여기에 대한 고유값과 고유벡터를 구합니다.  
고유값이 큰 순서대로 고유벡터를 선택하면, 그것이 주성분이며, 이 주성분 축으로 데이터를 투영하면, 가장 중요한 정보를 유지한 채, 차원을 줄일 수 있습니다. 