## 1. Overfitting이 무엇인지, 그리고 Overfitting을 방지하기 위한 방법으로 무엇이 있는지 설명해 보세요.  

**Overfitting**은 학습 데이터에는 잘 맞지만, 새로운 데이터에서는 성능이 떨어지는 현상입니다.  
즉, 모델이 데이터의 패턴뿐만 아니라, 노이즈까지 외워버려서, 일반화 능력이 떨어진 상태입니다.  
Overfitting을 방지 방법으로는 데이터 증강, 더 많은 데이터 확보, Dropout, Batch Normalization, L1/L2 Regularization, Early Stopping, Cross Validation 등이 있습니다.  

<br><br>

## 2. Dropout, Batch Normalization, L1/L2 Regularization, Cross Validation에 대해 설명해 보세요.  

**Dropout**은 학습 중 일부 뉴런을 임의로 비활성화하여, 특정 노드에 과도하게 의존하지 않도록 하는 방법입니다.  
**Batch Normalization**은 활성화 함수에 입력되기 전의 선형 변환된 결과를 정규화하여, 학습을 안정화시키는 방법입니다.   
**Regularization**은 모델의 복잡도를 제어하여 Overfitting을 방지하는 기법입니다. 
**L1 Regularization**은 가중치의 절대값 합에 패널티를 주어, 필요 없는 가중치를 줄이는 방식으로, 모델의 복잡도를 줄이는 방식이고,  
**L2 Regularization**은 가중치의 제곱합에 패널티를 주어, 가중치가 과도하게 커지는 것을 방지하는 방식으로, 모델의 복잡도를 줄이는 방식입니다.  
**Cross Validation**은 데이터를 여러 부분으로 나누고, 각 부분을 번갈아가며 validation set으로 사용하는 방식으로, 데이터 양이 적을 때 사용할 수 있는 방식입니다.  

<br><br>  

## 3. 활성화 함수에 대해 설명해 보세요.  

**활성화 함수**는 신경망에 비선형성을 부여하여, 더 큰 표현력을 갖게 하는 역할을 합니다.  
만약 활성화 함수가 없다면, 여러 층을 쌓더라도, 결국 선형 변환의 조합이 되어, 하나의 선형 변환과 동일한 표현력만 갖게 됩니다.  
대표적인 활성화 함수로 Sigmoid, ReLU 등이 있습니다.  

<br><br>  

## 4. 손실 함수에 대해 설명해 보세요.  

**손실 함수**는 모델의 예측 값과, 실제 값의 차이를 수치로 표현하는 함수입니다.  
학습 과정에서는 이 손실 함수의 값을 최도화하도록 파라미터를 업데이트하며,  
모델이 얼마나 잘 학습되고 있는지 평가하는 기준이 됩니다.  
주요 손실 함수로는 MSE, Cross-Entropy 등이 있습니다.  

<br><br>  

## 5-1. 역전파 알고리즘과 경사 하강법에 대해 설명해 보세요.  

**경사 하강법**은 손실 함수를 최소화하기 위한 최적화 기법으로, 손실 함수를 모델의 파라미터에 대해 미분해 얻은 기울기를 이용합니다.  
그렇게 얻은 **기울기**의 반대 방향으로 파라미터를 업데이트하여 손실을 점절 줄입니다.  
딥러닝에서는 이 기울기를 **역전파(Backpropagation)**를 통해 계산하며,  
업데이트 공식은 W = W - lr*(dL/dW)와 같이 학습률을 곱한 기울기를 빼주는 형태로 반복적으로 적용됩니다.   

<br><br>  

## 5-2. 경사 하강법의 장점에 대해 설명해 보세요.  

**경사 하강법**의 가장 큰 장점은 계산이 단순하면서도, 고차원 파라미터 공간에서 효율적으로 동작한다는 점입니다.  
손실함수를 파라미터로 미분한 기울기만 알면 되기 떄문에, 수치 미분처럼 여러 번 함수를 계산할 필요 없이, 한 번의 기울기 계산으로 파라미터를 빠르게 업데이트 할 수 있습니다.   
또한, 경사 하강법은 모델 구조와 무관하게 적용할 수 있어, 선형 회귀부터 신경망까지 다양한 모델을 학습시키는데 사용됩니다.  

<br><br>

## 6. 경사 하강법의 변형 방법 (예: SGD, Adam)에 대해 설명해 보세요.  

경사 하강법은 전체 데이터의 기울기를 사용해 계산하는데, 연산량을 줄이고 학습 속도를 높이기 위한 여러 최적화 방법이 있습니다.  
SGD는 전체 데이터가 아니라, 일부 미니배치 데이터의 기울기를 사용해, 연산량을 줄이고 학습 속도를 높이는 방법입니다.  
Adam은 이전 기울기의 평균과 분산 등 정보를 함께 고려해, 각 파라미터별로 학습률을 자동 조정하는 방법입니다.  
이로 인해 학습이 안정적이고, 학습 속도가 빠르다는 장점이 있습니다.  

<br><br>  

## 7. Gradient Vanishing과 Gradient Exploding에 대해 설명해 보세요.  

**Gradient Vanishing**은 네트워크의 깊이가 깊어질수록, 기울기가 작아져 학습이 어려워지는 문제입니다.  
역전파 과정에서, chain rule에 의해 기울기가 연속적으로 곱해지게 됩니다.  
그런데, 활성화 함수의 미분값이 1보다 작은 경우, 기울기가 연속적으로 곱해지게 되면, 초기층으로 전달되는 기울기는 거의 0에 가까워지게 됩니다.  
그러면 파라미터가 업데이트되지 않고, 학습이 멈추는 현상이 발생합니다.  
**Gradient Exploding**은 가중치가 너무 커져서, 학습이 불안정해지는 문제입니다.  

<br><br>  

## 8. 편향(Bias)과 분산(Variance)의 개념을 설명하고, 이 두가지가 모델의 성능에 미치는 영향에 대해 설명해 보세요.  

**Bias**는 모델이 훈련 데이터의 실제 관계를 제대로 학습하지 못해 생기는 오류를 의미하며, 단순한 모델일 수록 Bias가 높아져 underfitting이 발생합니다.  
**Variance**는 모델이 훈련 데이터의 작은 변화에도 민감하게 반응하는 정도를 의미하며, 복잡한 모델일 수록 Variance가 높아져 overfitting이 발생합니다.  


<br><br>  

## 9. 시그모이드 함수에서 기울기 소실이 일어나는 이유를 설명해 보세요.    

**시그모이드**는 입력이 아주 크거나 작으면, 기울기가 0에 가까워지고, x=0에서 기울기의 최댓값이 0.25에 불과합니다.  
역전파에서는 각 층의 기울기가 계속 곱해지기 때문에, 0.25보다 작은 값이 누적되면서, 앞쪽 레이어로 갈수록 기울기가 0에 가까워집니다.  
그래서 깊은 신경망에서는 기울기 소실이 발생하게 됩니다.  

<br><br>  

## 10. SVM의 기본 원리와 커널 트릭의 역할을 설명해 보세요.  

**SVM**은 데이터를 선형적으로 분리하는 최적의 결정 경계를 찾는 분류 알고리즘입니다.  
주요 아이디어로는 데이터 포인트와 결정 경계간의 최대 마진을 찾는 것입니다.  
**커널 트릭**은 비선형적으로 분리된 데이터를, 고차원 공간으로 배핑하여, 선형적으로 분리 가능하게 만드는 방법입니다.  

<br><br>  

## 11. PCA에 대해 설명해 보세요.  

**PCA**는 고차원 데이터를 저차원으로 축소할 때, 정보 손실을 최소화하기 위해, 분산이 가장 큰 방향을 찾아, 그 방향으로 데이터를 투영하는 방법입니다.  
이를 위해 먼저 전체 feature 쌍 간의 공분산 행렬을 계산한 뒤, 여기에 대한 고유값과 고유 벡터를 구합니다.  
고유값이 큰 순서대로 고유 벡터를 선택하면 그것이 주성분이며, 이 주성분 축으로 데이터를 투영하면 가장 중요한 정보를 유지한 채 차원을 줄일 수 있습니다.   

<br><br>  

## 12. Learning Rate Schedule에 대해 설명해 보세요.  

**Learning Rate Schedule**은 학습이 진행됨에 따라 Learning Rate를 조정하는 방법입니다.  
학습 초기에는 큰 학습률로 빠르게 수렴 방향을 잡고,    
이후에는 점점 학습률을 줄여서 최적점 근처에서 안정적으로 수렴하도록 합니다.  

<br><br>  